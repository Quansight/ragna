{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192e7b22-47c8-41f5-ae1b-f63b0ac5131d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7646cdb-fadc-4e57-ad96-05532a211bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai work\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "def stream_openai(client, role, message, model=\"gpt-4o-mini\"):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": role, \"content\": message}],\n",
    "        stream=True,\n",
    "    )\n",
    "    message = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            message += chunk.choices[0].delta.content\n",
    "    return message\n",
    "\n",
    "# m = stream_openai(client, \"user\", \"hello, how are you?\")\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb94b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor test\n",
    "from pathlib import Path\n",
    "\n",
    "from ragna.source_storages import RagnaDemoSourceStorage\n",
    "from ragna.core import (\n",
    "    LocalDocument, Document\n",
    ")\n",
    "\n",
    "\n",
    "storage = RagnaDemoSourceStorage()\n",
    "\n",
    "# md_files = \n",
    "# ragna_documents = [\n",
    "#             (\n",
    "#                 document\n",
    "#                 if isinstance(document, Document)\n",
    "#                 else LocalDocument.from_path(document)\n",
    "#             )\n",
    "#             for document in md_files\n",
    "#         ]\n",
    "# storage.store(\"ragna_docs\", ragna_documents)\n",
    "\n",
    "temporal_doc_paths = [p for p in Path.cwd().joinpath(\"temporal_docs\").iterdir() if p.suffix == \".md\"]\n",
    "temporal_docs = [\n",
    "    LocalDocument.from_path(path) for path in temporal_doc_paths\n",
    "]\n",
    "storage.store(\"temporal_docs\", temporal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97926ef8",
   "metadata": {},
   "source": [
    "## Run ragna with no preprocessing on temporal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861713ce-bb5f-484b-b7e3-e96b9b8775ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last year, in 2023, a rocket was built.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from ragna.assistants import RagnaDemoAssistant\n",
    "\n",
    "from ragna import Rag\n",
    "from ragna.assistants import Gpt4\n",
    "\n",
    "\n",
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"temporal_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(\"What happened last year?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9af3eb-5efe-4d17-875d-5480f928c94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85129b4-e087-4de7-9d30-a23f0f849f25",
   "metadata": {},
   "source": [
    "## Run with Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6aebb98-a054-4528-967a-d46dbd96bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from ragna.core import (\n",
    "QueryPreprocessor, MetadataFilter, ProcessedQuery\n",
    ")\n",
    "\n",
    "base_prompt = (\n",
    "\"\"\"You are a llm agent that is responsible for rewriting an input prompt for a RAG application. \n",
    "A question might contain hidden context that may not be recognized by the embedding model. \n",
    "An embedding of a question might not generate a close match to an embedding of a statement\n",
    "that contains the answer to the question. Although this is technically something that should\n",
    "be solved on the embedding model side, it is usually solved by rephrasing the question\n",
    "before using it to retrieve sources. For example a question like \"What happened last month?\" \n",
    "likely won't get any close matches. Rephrasing the prompt to \"What happened in December 2021?\"\n",
    "given that the question is asked January 2022. Things that may be important to consider when \n",
    "reworking the prompt would be the current context, that may that is not explicitly asked in the \n",
    "question but can be inferred, for instance the current date. \n",
    "\n",
    "current date: 2025-01-24\n",
    "\n",
    "\n",
    "Please reword the following prompt \n",
    "and only return the reworded prompt. I do not need to know your reasoning:\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "class TestPreprocessor(QueryPreprocessor):\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.storage=storage\n",
    "        # self.assistant=assistant\n",
    "        self.messages = []\n",
    "\n",
    "    def ask_assistant(self, prompt):\n",
    "        instruction = (base_prompt + prompt)\n",
    "        print(instruction)\n",
    "        assistant_answer = stream_openai(client, \"user\", instruction)\n",
    "        return assistant_answer\n",
    "\n",
    "    def process(self, query: str, metadata_filter: Optional[MetadataFilter]):\n",
    "        processed_query = self.ask_assistant(query)\n",
    "        return ProcessedQuery(\n",
    "            original_query=query,\n",
    "            processed_query=processed_query,\n",
    "            metadata_filter=None,\n",
    "            processor_name=self.display_name()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4879d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a llm agent that is responsible for rewriting an input prompt for a RAG application. \n",
      "A question might contain hidden context that may not be recognized by the embedding model. \n",
      "An embedding of a question might not generate a close match to an embedding of a statement\n",
      "that contains the answer to the question. Although this is technically something that should\n",
      "be solved on the embedding model side, it is usually solved by rephrasing the question\n",
      "before using it to retrieve sources. For example a question like \"What happened last month?\" \n",
      "likely won't get any close matches. Rephrasing the prompt to \"What happened in December 2021?\"\n",
      "given that the question is asked January 2022. Things that may be important to consider when \n",
      "reworking the prompt would be the current context, that may that is not explicitly asked in the \n",
      "question but can be inferred, for instance the current date. \n",
      "\n",
      "current date: 2025-01-24\n",
      "\n",
      "\n",
      "Please reword the following prompt \n",
      "and only return the reworded prompt. I do not need to know your reasoning:\n",
      "\n",
      "\n",
      "What happened two months ago?\n"
     ]
    }
   ],
   "source": [
    "preprocessor = TestPreprocessor()\n",
    "processed_query = preprocessor.process(\"What happened last year?\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbbf236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happened in November 2024?\n"
     ]
    }
   ],
   "source": [
    "print(processed_query.processed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c92e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2024, they went to the moon.\n"
     ]
    }
   ],
   "source": [
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"temporal_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(processed_query.processed_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0920d",
   "metadata": {},
   "source": [
    "# query preprocessor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84da6383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/andrew/.dropbox-hm/Dropbox/quansight/dev/ragna/ragna/preprocessing_demo/../docs/install.md'), PosixPath('/home/andrew/.dropbox-hm/Dropbox/quansight/dev/ragna/ragna/preprocessing_demo/../docs/index.md')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/micromamba/envs/ragna-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Ragna docs\n",
    "from ragna.source_storages import LanceDB\n",
    "\n",
    "docs_path = Path.cwd().joinpath(\"../docs\")\n",
    "\n",
    "md_files = list(docs_path.glob(\"**/*.md\"))\n",
    "print(md_files[:2])\n",
    "\n",
    "storage = LanceDB()\n",
    "\n",
    "documents = [\n",
    "            (\n",
    "                document\n",
    "                if isinstance(document, Document)\n",
    "                else LocalDocument.from_path(document)\n",
    "            )\n",
    "            for document in md_files\n",
    "        ]\n",
    "storage.store(\"ragna_docs\", documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65b23e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"how do I build a rag application?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be46c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the information provided does not include details on how to build a RAG application.\n"
     ]
    }
   ],
   "source": [
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"ragna_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(example_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ade1b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from ragna.core import (\n",
    "QueryPreprocessor, MetadataFilter, ProcessedQuery\n",
    ")\n",
    "\n",
    "base_prompt = (\n",
    "\"\"\"You are a helpful software engineer. Hypothetically given a specific software package in python,\n",
    " how would you answer the following question from a more junior developer?\n",
    "\n",
    " question:\n",
    "\"\"\")\n",
    "\n",
    "class QueryExpansionPreprocessor(QueryPreprocessor):\n",
    "\n",
    "    def __init__(self, storage):\n",
    "        self.storage=storage\n",
    "        # self.assistant=assistant\n",
    "        self.messages = []\n",
    "\n",
    "    def ask_assistant(self, prompt):\n",
    "        instruction = (base_prompt + prompt)\n",
    "        assistant_answer = stream_openai(client, \"user\", instruction)\n",
    "        return assistant_answer\n",
    "\n",
    "    def process(self, query: str, metadata_filter: Optional[MetadataFilter]):\n",
    "        hypothetical_answer = self.ask_assistant(query)\n",
    "        sources = self.storage.retrieve(\"ragna_docs\", metadata_filter, hypothetical_answer)\n",
    "        \n",
    "        processed_query = \"\\n\".join(s.content for s in sources) + \"\\n\\n\" + query\n",
    "        return ProcessedQuery(\n",
    "            original_query=query,\n",
    "            processed_query=processed_query,\n",
    "            metadata_filter=None,\n",
    "            processor_name=self.display_name()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "00433eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessedQuery(original_query='how do I build a rag application?', processed_query='# Frequently asked questions\\n\\n## Why should I use Ragna and not X?\\n\\n!!! tip \"TL;DR\"\\n\\n    Ragna is the only tool out there that specializes in orchestrating RAG use cases\\n    with arbitrary components, as well as offering a Python API, a REST API, and a web\\n    UI for that.\\n\\n!!! note\\n\\n    Although we try to be objective as possible, this section is inheritly biased. If\\n    you are the author of a package we don\\'t mention below but think we should or your\\n    package is mentiond but you feel we have mischaracterized it, please\\n    [get in touch](https://github.com/Quansight/ragna/discussions).\\n\\nAfter the emergence of ChatGPT in November of 2022, the field of LLMs exploded. Today,\\nthere are many providers for LLM REST APIs out there. With these, we also have a\\nplethora of Python packages to build applications around the provided APIs. We cannot\\nsummarize the whole field here, so we stick to large projects in the Python ecosystem\\nfor comparison.\\n\\n| library or application                                |        RAG         | arbitrary components |     Python API     |      REST API      |       web UI       |\\n| ----------------------------------------------------- | :----------------: | :------------------: | :----------------: | :----------------: | :----------------: |\\n| Ragna                                                 | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\\n| [LangChain](https://www.langchain.com/)               | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [Langroid](https://langroid.github.io/langroid/)      | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [LlamaIndex](https://www.llamaindex.ai/)              | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [Ollama](https://ollama.com/)                         |        :x:         |         :x:          | :heavy_check_mark: | :heavy_check_mark: |\\n Internally, this is roughly treated as\\n`from ragna.core import LocalDocument`.\\n\\nYou can inject your own objects here and do not need to rely on the defaults by Ragna.\\nTo do so, make sure that the module the object is defined in is on\\n[Python\\'s search path](https://docs.python.org/3/library/sys.html#sys.path). There are\\nmultiple ways to achieve this, e.g.:\\n\\n- Install your module as part of a package in your current environment.\\n- Set the [`PYTHONPATH`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH)\\n  environment variable to include the directory your module is located in.\\n\\n## Environment variables\\n\\nAll configuration options can be set or overridden by environment variables by using the\\n`RAGNA_` prefix. For example, `document = ragna.core.LocalDocument` in the configuration\\nfile is equivalent to setting `RAGNA_DOCUMENT=ragna.core.LocalDocument`.\\n\\nFor configuration options in subsections, the subsection name needs to be appended to\\nthe prefix, e.g. `RAGNA_API_`. The value needs to be in JSON format. For example\\n\\n```toml\\n[api]\\norigins = [\\n    \"http://localhost:31477\",\\n]\\n```\\n\\nis equivalent to `RAGNA_API_ORIGINS=\\'[\"http://localhost:31477\"]\\'`.\\n\\n## Configuration options\\n\\n### `local_root`\\n\\nLocal root directory Ragna uses for storing files. See [ragna.local_root][].\\n\\n### `auth`\\n\\n[ragna.deploy.Auth][] class to use for authenticating users.\\n\\n### `key_value_store`\\n\\n[ragna.deploy.KeyValueStore][] class to use for temporary storage.\\n\\n### `document`\\n\\n[ragna.core.Document][] class to use to upload and read documents.\\n\\n### `source_storages`\\n\\n[ragna.core.SourceStorage][]s to be available for the user to use.\\n\\n### `assistants`\\n\\n[ragna.core.Assistant][]s to be available for the user to use.\\n\\n### `hostname`\\n\\nHostname the REST API will be bound to.\\n\\n### `port`\\n\\nPort the REST API will be bound to.\\n\\n### `root_path`\\n\\nA path prefix handled by a proxy that is not seen by the REST API, but is seen by\\nexternal clients.\\n\\n### `origins`\\n\\n[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) origins that are allowed\\nto connect to the REST\\n\\nhow do I build a rag application?', metadata_filter=None, processing_history=[])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = QueryExpansionPreprocessor(storage)\n",
    "\n",
    "preprocessed_query = preprocessor.process(example_question, None)\n",
    "preprocessed_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "630fb707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessedQuery(original_query='how do I build a rag application?', processed_query='# Frequently asked questions\\n\\n## Why should I use Ragna and not X?\\n\\n!!! tip \"TL;DR\"\\n\\n    Ragna is the only tool out there that specializes in orchestrating RAG use cases\\n    with arbitrary components, as well as offering a Python API, a REST API, and a web\\n    UI for that.\\n\\n!!! note\\n\\n    Although we try to be objective as possible, this section is inheritly biased. If\\n    you are the author of a package we don\\'t mention below but think we should or your\\n    package is mentiond but you feel we have mischaracterized it, please\\n    [get in touch](https://github.com/Quansight/ragna/discussions).\\n\\nAfter the emergence of ChatGPT in November of 2022, the field of LLMs exploded. Today,\\nthere are many providers for LLM REST APIs out there. With these, we also have a\\nplethora of Python packages to build applications around the provided APIs. We cannot\\nsummarize the whole field here, so we stick to large projects in the Python ecosystem\\nfor comparison.\\n\\n| library or application                                |        RAG         | arbitrary components |     Python API     |      REST API      |       web UI       |\\n| ----------------------------------------------------- | :----------------: | :------------------: | :----------------: | :----------------: | :----------------: |\\n| Ragna                                                 | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\\n| [LangChain](https://www.langchain.com/)               | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [Langroid](https://langroid.github.io/langroid/)      | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [LlamaIndex](https://www.llamaindex.ai/)              | :heavy_check_mark: |  :heavy_check_mark:  | :heavy_check_mark: |        :x:         |        :x:         |\\n| [Ollama](https://ollama.com/)                         |        :x:         |         :x:          | :heavy_check_mark: | :heavy_check_mark: |\\n Internally, this is roughly treated as\\n`from ragna.core import LocalDocument`.\\n\\nYou can inject your own objects here and do not need to rely on the defaults by Ragna.\\nTo do so, make sure that the module the object is defined in is on\\n[Python\\'s search path](https://docs.python.org/3/library/sys.html#sys.path). There are\\nmultiple ways to achieve this, e.g.:\\n\\n- Install your module as part of a package in your current environment.\\n- Set the [`PYTHONPATH`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH)\\n  environment variable to include the directory your module is located in.\\n\\n## Environment variables\\n\\nAll configuration options can be set or overridden by environment variables by using the\\n`RAGNA_` prefix. For example, `document = ragna.core.LocalDocument` in the configuration\\nfile is equivalent to setting `RAGNA_DOCUMENT=ragna.core.LocalDocument`.\\n\\nFor configuration options in subsections, the subsection name needs to be appended to\\nthe prefix, e.g. `RAGNA_API_`. The value needs to be in JSON format. For example\\n\\n```toml\\n[api]\\norigins = [\\n    \"http://localhost:31477\",\\n]\\n```\\n\\nis equivalent to `RAGNA_API_ORIGINS=\\'[\"http://localhost:31477\"]\\'`.\\n\\n## Configuration options\\n\\n### `local_root`\\n\\nLocal root directory Ragna uses for storing files. See [ragna.local_root][].\\n\\n### `auth`\\n\\n[ragna.deploy.Auth][] class to use for authenticating users.\\n\\n### `key_value_store`\\n\\n[ragna.deploy.KeyValueStore][] class to use for temporary storage.\\n\\n### `document`\\n\\n[ragna.core.Document][] class to use to upload and read documents.\\n\\n### `source_storages`\\n\\n[ragna.core.SourceStorage][]s to be available for the user to use.\\n\\n### `assistants`\\n\\n[ragna.core.Assistant][]s to be available for the user to use.\\n\\n### `hostname`\\n\\nHostname the REST API will be bound to.\\n\\n### `port`\\n\\nPort the REST API will be bound to.\\n\\n### `root_path`\\n\\nA path prefix handled by a proxy that is not seen by the REST API, but is seen by\\nexternal clients.\\n\\n### `origins`\\n\\n[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) origins that are allowed\\nto connect to the REST\\n\\nhow do I build a rag application?', metadata_filter=None, processing_history=[])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a4703bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build a RAG (Retrieval-Augmented Generation) application using Ragna, you would need to follow these steps:\n",
      "\n",
      "1. Install Ragna: You can install Ragna using pip, a package installer for Python. You can do this by running the command `pip install ragna` in your terminal.\n",
      "\n",
      "2. Set up your configuration: Ragna uses a configuration file to set up the environment. This includes setting up the local root directory for storing files, the authentication class for user authentication, the key-value store class for temporary storage, and the document class for uploading and reading documents. You can also set up the source storages and assistants available for the user to use.\n",
      "\n",
      "3. Write your RAG components: You would need to write your own RAG components, which include the document, source storage, and assistant classes. These classes should inherit from the respective base classes in Ragna and implement the required methods.\n",
      "\n",
      "4. Run your application: Once you have set up your configuration and written your RAG components, you can run your application. If you are using the REST API, you would need to set up the hostname, port, and root path for the API.\n",
      "\n",
      "Please note that this is a high-level overview of the process. The exact steps\n"
     ]
    }
   ],
   "source": [
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"ragna_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(preprocessed_query.processed_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragna-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
