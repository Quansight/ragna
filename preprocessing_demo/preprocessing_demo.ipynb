{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192e7b22-47c8-41f5-ae1b-f63b0ac5131d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7646cdb-fadc-4e57-ad96-05532a211bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai work\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "def stream_openai(client, role, message):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": role, \"content\": message}],\n",
    "        stream=True,\n",
    "    )\n",
    "    message = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            message += chunk.choices[0].delta.content\n",
    "    return message\n",
    "\n",
    "# m = stream_openai(client, \"user\", \"hello, how are you?\")\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb94b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor test\n",
    "from pathlib import Path\n",
    "\n",
    "from ragna.source_storages import RagnaDemoSourceStorage\n",
    "from ragna.core import (\n",
    "    LocalDocument, Document\n",
    ")\n",
    "\n",
    "\n",
    "storage = RagnaDemoSourceStorage()\n",
    "\n",
    "# md_files = \n",
    "# ragna_documents = [\n",
    "#             (\n",
    "#                 document\n",
    "#                 if isinstance(document, Document)\n",
    "#                 else LocalDocument.from_path(document)\n",
    "#             )\n",
    "#             for document in md_files\n",
    "#         ]\n",
    "# storage.store(\"ragna_docs\", ragna_documents)\n",
    "\n",
    "temporal_doc_paths = [p for p in Path.cwd().joinpath(\"temporal_docs\").iterdir() if p.suffix == \".md\"]\n",
    "temporal_docs = [\n",
    "    LocalDocument.from_path(path) for path in temporal_doc_paths\n",
    "]\n",
    "storage.store(\"temporal_docs\", temporal_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97926ef8",
   "metadata": {},
   "source": [
    "## Run ragna with no preprocessing on temporal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861713ce-bb5f-484b-b7e3-e96b9b8775ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last year, in 2023, a rocket was built.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from ragna.assistants import RagnaDemoAssistant\n",
    "\n",
    "from ragna import Rag\n",
    "from ragna.assistants import Gpt4\n",
    "\n",
    "\n",
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"temporal_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(\"What happened last year?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9af3eb-5efe-4d17-875d-5480f928c94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85129b4-e087-4de7-9d30-a23f0f849f25",
   "metadata": {},
   "source": [
    "## Run with Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6aebb98-a054-4528-967a-d46dbd96bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from ragna.core import (\n",
    "QueryPreprocessor, MetadataFilter, ProcessedQuery\n",
    ")\n",
    "\n",
    "base_prompt = (\n",
    "\"\"\"You are a llm agent that is responsible for rewriting an input prompt for a RAG application. \n",
    "A question might contain hidden context that may not be recognized by the embedding model. \n",
    "An embedding of a question might not generate a close match to an embedding of a statement\n",
    "that contains the answer to the question. Although this is technically something that should\n",
    "be solved on the embedding model side, it is usually solved by rephrasing the question\n",
    "before using it to retrieve sources. For example a question like \"What happened last month?\" \n",
    "likely won't get any close matches. Rephrasing the prompt to \"What happened in December 2021?\"\n",
    "given that the question is asked January 2022. Things that may be important to consider when \n",
    "reworking the prompt would be the current context, that may that is not explicitly asked in the \n",
    "question but can be inferred, for instance the current date. \n",
    "\n",
    "current date: 2025-01-24\n",
    "\n",
    "\n",
    "Please reword the following prompt \n",
    "and only return the reworded prompt. I do not need to know your reasoning:\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "class TestPreprocessor(QueryPreprocessor):\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.storage=storage\n",
    "        # self.assistant=assistant\n",
    "        self.messages = []\n",
    "\n",
    "    def ask_assistant(self, prompt):\n",
    "        instruction = (base_prompt + prompt)\n",
    "        print(instruction)\n",
    "        assistant_answer = stream_openai(client, \"user\", instruction)\n",
    "        return assistant_answer\n",
    "\n",
    "    def process(self, query: str, metadata_filter: Optional[MetadataFilter]):\n",
    "        processed_query = self.ask_assistant(query)\n",
    "        return ProcessedQuery(\n",
    "            original_query=query,\n",
    "            processed_query=processed_query,\n",
    "            metadata_filter=None,\n",
    "            processor_name=self.display_name()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4879d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a llm agent that is responsible for rewriting an input prompt for a RAG application. \n",
      "A question might contain hidden context that may not be recognized by the embedding model. \n",
      "An embedding of a question might not generate a close match to an embedding of a statement\n",
      "that contains the answer to the question. Although this is technically something that should\n",
      "be solved on the embedding model side, it is usually solved by rephrasing the question\n",
      "before using it to retrieve sources. For example a question like \"What happened last month?\" \n",
      "likely won't get any close matches. Rephrasing the prompt to \"What happened in December 2021?\"\n",
      "given that the question is asked January 2022. Things that may be important to consider when \n",
      "reworking the prompt would be the current context, that may that is not explicitly asked in the \n",
      "question but can be inferred, for instance the current date. \n",
      "\n",
      "current date: 2025-01-24\n",
      "\n",
      "\n",
      "Please reword the following prompt \n",
      "and only return the reworded prompt. I do not need to know your reasoning:\n",
      "\n",
      "\n",
      "What happened last year?\n"
     ]
    }
   ],
   "source": [
    "preprocessor = TestPreprocessor()\n",
    "processed_query = preprocessor.process(\"What happened last year?\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbbf236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happened in 2024?\n"
     ]
    }
   ],
   "source": [
    "print(processed_query.processed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c92e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2024, they went to the moon.\n"
     ]
    }
   ],
   "source": [
    "chat = Rag().chat(\n",
    "    input=None,\n",
    "    source_storage = storage,\n",
    "    #assistant=RagnaDemoAssistant,\n",
    "    assistant=Gpt4,\n",
    "    corpus_name=\"temporal_docs\",\n",
    ")\n",
    "_ = await chat.prepare()\n",
    "print(await chat.answer(processed_query.processed_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragna-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
